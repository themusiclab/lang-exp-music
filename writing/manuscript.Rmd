---
header-includes:
- \usepackage[left]{lineno}
- \usepackage{mdframed}
- \usepackage{soul}
- \usepackage{xcolor}
- \definecolor{bleu}{HTML}{2200cc}
- \usepackage{caption}
- \usepackage{ragged2e}
- \usepackage{afterpage}
- \usepackage[labelformat = empty]{caption}
- \usepackage{rotating}
output: 
  pdf_document:
    fig_caption: yes
    number_sections: true
    latex_engine: xelatex
csl: apa.csl
bibliography: "themusiclab.bib"
nocite: "@Bailey1988; @vanderHulst2010; @Inkelas1988; @Chen2016; Ngo2016; @Swaminathan2021; @Choi2021; @Evans2018a; @Jefferies1990; @Niesler2005; @Manyah2006; @Eme2022; @Westermeyer1977; @Hamann2015; @Goldsmith2011; @Ameka2001"
urlcolor: bleu
linkcolor: bleu
---


```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)
options(scipen = 999)

# Note: producing all the matched and inverse-probability weighted datasets takes a long time and a lot of memory for the computations to run, so this rmd only runs presaved outputs. But if you want to run these analysis steps set full_run below to TRUE

full_run <- FALSE
if (full_run) {
  source(here("analysis", "subsampling.R"))
  source(here("analysis", "analyses.R"))
  knitr::knit(here("viz", "figures.Rmd"))
}

```

```{r load-library}

library(pacman)
p_load(
  broom,
  knitr,
  ggridges,
  ggtext,
  scales,
  lmerTest,
  broom.mixed,
  patchwork,
  gghighlight,
  kableExtra,
  ggrepel,
  jtools, 
  here,
  tidyverse,
  survey,
  ggeffects,
  emmeans,
  meta,
  latex2exp
)

color_scheme <- c("#85C0F9", "#A95AA1", "#F5793A")

# short alias for adding commas to big numbers
f <- partial(prettyNum, big.mark = ",")

# format p-values
fp <- function(num) {
  if (num < .0001) {
    return("< 0.001")
  } else {
    return(paste0("= ", round(num, digits = 3)))
  }
}

```

```{r load-analyses}
load(here("results", "analyses.RData"))
load(here("results", "meta_analyses.RData"))
load(here("results", "permutation_tests.RData"))
miq_combined_filtered <- read.csv(here("data", "Combined", "Combined_filtered.csv"))
headphone_score <- read.csv(here("data", "headphone_score.csv"))
```

<!--
Tables and figures...

Fig 1. Sample sizes (word plot)
Fig 2. Main effects summarized w/ music-lesson as reference point
Fig 3. Language-wise forestplots
Table 1. Main effects statistical reporting (mixed models)

SI
Text 1.1 Meta-analysis
Text 1.2 Headphone check
Text 1.3 Preregistered version of everything

Fig S1. Headphone check scores

Table S1. All languages, samples etc
Table S2. Demographics
Table S3. Income-controlled US model
Table S4. Education-controlled US model
Table S5. East v. West model
Table S6A & S6B. Exploratory analysis 
-->


\raggedright
\LARGE 
\begin{center}
\textbf{Language experience predicts music processing\\ in $\frac{1}{2}$ million speakers of 54 languages}
\end{center}


\vspace{0.2in}

\justifying
\normalsize
Jingxuan Liu$^{1,2,\wedge,\ast}$, Courtney B. Hilton$^{3,4\wedge,\ast}$, Elika Bergelson$^{2}$, & Samuel A. Mehr$^{3,4,\ast}$

\small

$^{1}$Columbia Business School, Columbia University, New York, NY 10027, USA \
$^{2}$Department of Psychology \& Neuroscience, Duke University, Durham, NC 27708, USA \
$^{3}$Haskins Laboratories, Yale University, New Haven, CT 06511, USA \
$^{4}$School of Psychology, University of Auckland, Auckland 1010, New Zealand \

$^{\wedge}$These authors contributed equally.


\*Corresponding author. E-mails: [jl6297\@gsb.columbia.edu](mailto:jl6297@gsb.columbia.edu){.email}, [courtney.hilton@auckland.ac.nz](mailto:courtney.hilton@auckland.ac.nz){.email}, [sam\@yale.edu](mailto:sam@yale.edu){.email} 

\bigskip
\normalsize

```{=tex}
\begin{mdframed}[backgroundcolor=gray!20]
Tonal languages differ from other languages in their use of pitch (tones) to distinguish words. Lifelong experience speaking and hearing tonal languages has been argued to shape auditory processing in ways that generalize beyond the perception of \textit{linguistic} pitch to the perception of pitch in other domains like \textit{music}. To examine this, we first conducted a meta-analysis whose results were consistent with this idea, but limited by mostly small sample sizes in only a few tonal languages and countries, which made it challenging to disentangle the effects of linguistic experience from variability in music training experience, cultural differences, and other potential confounds. To address these issues, we used web-based citizen science to test this question on a global scale. We assessed music perception skill in $n = `r f(users$speakers$Tonal)`$ native speakers of `r f(users$langs$Tonal)` tonal languages and compared their performance to $n = `r f(users$speakers$other)`$ native speakers of other languages, including `r f(users$langs$Pitch_accented)` pitch-accented and `r f(users$langs$Non_tonal)` non-tonal languages (e.g. Mandarin, Yoruba; Japanese; Hungarian, respectively). Whether or not participants had taken music lessons, native speakers of all 19 tonal languages had an improved ability to discriminate musical melodies. But this improvement came with a trade-off: relative to speakers of pitch-accented or non-tonal languages, tonal language speakers were also worse at processing the musical beat. These results, which held across tonal languages from a variety of geographic regions and were robust to geographic and demographic variation, demonstrate that linguistic experience shapes music perception ability, with implications for relations between music, language, and culture in the human mind.
\end{mdframed}
```

\linenumbers

# Introduction

From infancy and early childhood, we are surrounded by people speaking and singing [@Bergelson2019; @Eibl-Eibesfeldt1979; @Mehr2019; @Mehr2020; @Bonneville-Roussy2013; @Konner2010; @Mendoza2021; @Mehr2014; @Yan2021]. This immersion continues throughout the lifespan and is reinforced through our own language and music production. 

Human perception readily adapts to these soundscapes: early speech experiences tune our hearing to the speech contrasts of our native language(s) [@Kuhl2004; @Werker1984; @Polka1994], and musical experiences during the same time period are thought to have similar "perceptual narrowing" effects, biasing listeners' interpretations of musical rhythm and pitch based on their own musical cultures [@Hannon2005; @Lynch1990]. These effects may cross domains. While music training has minimal causal effects on high-level cognitive skills [@Sala2020; @Mehr2013a], it may sharpen lower-level aspects of speech processing [@Patel2011; @Wong2007] and auditory perception [@Kraus2010]. In the opposite direction, enhanced experience with the types of linguistic pitch used in tonal languages has been argued to shape pitch processing in music [@Bradley2016; @Bidelman2013; @Pfordresher2009]. 

Here, we study the latter possibility, to examine the effects of language experience on music processing, with a focus on pitch. Languages can be classified into three distinct categories based on their use of pitch: tonal, non-tonal, and pitch-accented. While all spoken languages convey information via pitch, tonal languages, which represent over half the world's languages [including many East Asian, Southeast Asian and African languages\; @Yip2002] use pitch in a special fashion. In tonal languages, pitch is often used lexically: speaking the same syllable with a different pitch level or shape alters meaning at the word level [@Pike1948; @vanderHulst2011]. A canonical example is the Mandarin syllable $ma$, which has different meanings depending on its tonal contour (i.e., level, rising, falling-rising, or falling). This property requires pitch sensitivity in both speakers and listeners, lest one scold ($m\grave{a}$) one's mother ($m\bar{a}$) instead of one's horse ($m\breve{a}$).

The lexical use of pitch in tonal languages is distinct from how pitch is otherwise used in speech. For example, many languages use pitch to convey affect [@Cowen2019]; to cue non-lexical meaning [e.g., helping to differentiate between questions and statements\; @Patel2008; @Tong2005]; to emphasize information [@Breen2010]; to cue sentence structure with metrical stress patterns [@Wagner2019], supporting comprehension [@Hilton2021b]; and/or as a cue to speech categories, as in infant- or child-directed speech [@Hilton2022a]. While these many uses of pitch are typical of speech in both tonal and non-tonal languages [e.g., many Indo-European, South Asian, or Australian languages\; @Maddieson2013], in non-tonal languages, pitch is never used lexically to denote word meanings. Lastly, *Pitch-accented* languages is an intermediate category with limited or mixed use of lexical pitch [such as Croatian\; @vanderHulst2011]; note, however, that whether pitch-accented languages form a coherent standalone category or whether they are better considered on a spectrum between tonal and non-tonal languages, with some mixed cases, is a matter of debate [e.g., @Gussenhoven2004; @Hyman2006; @Hyman2009].^[Indeed, a category of languages that groups evidently disparate languages together, such as Japanese and Swedish, may not be defensible. As pitch-accented languages are not our primary focus, here we treat them as a separate group from tonal and non-tonal languages, but also conduct some analyses at the language level rather than the language-type level. <!--We encourage anyone interested in alternative groupings of languages to use our public data and code to re-analyze accordingly.-->]

The special role of pitch in tonal languages has motivated the hypothesis that speaking a tonal language sharpens pitch perception in a domain-general fashion. Indeed, compared to speakers of non-tonal languages, native speakers of at least some East and/or Southeast Asian tonal languages not only better discriminate the tones of their native language and those of other tonal languages they do not speak [@Krishnan2010; @Li2018; @Peng2010], but also may have stronger categorical perception for non-speech pitch patterns generally. Speakers of East/Southeast Asian tonal languages also have distinct neural responses to pitch in brain areas associated with early auditory processing [@Bidelman2011; @Bidelman2011a; @Bidelman2015; @Krishnan2005; @Krishnan2010].

Might domain-general auditory processing advantages transfer to enhanced pitch processing in music? Many studies have tested this question by comparing native speakers of tonal and non-tonal languages on a variety of musical pitch perception tasks. Some studies report that tonal language speakers excel at discriminating melodic patterns [@Alexander2008; @Bradley2016; @Choi2021; @Chen2016; @Ngo2016; @Swaminathan2021; @Wong2012]; or at discerning fine-grained pitch difference either in isolation or in the context of detuned intervals, contours, and melodies [@Bidelman2013; @Chen2016; @Giuliano2011; @Hutka2015]. But other studies fail to replicate these patterns, both for melodic discrimination [@Peretz2011; @Zheng2018; @Stevens2013] and fine-scale pitch discrimination [@Bent2006; @Bidelman2011a; @Pfordresher2009; @Stevens2013; @Tong2018; @Wong2012]. Some studies even find that tonal language speakers have *more* trouble distinguishing musical pitch contours, suggesting that lexical tone experience could interfere with pitch perception in some contexts [@Bent2006; @Chang2016; @Peretz2011; @Zheng2018]. 

More generally, because the vast majority of participants in these studies were native speakers of a small number of tonal and non-tonal languages from two non-overlapping geographic areas (East Asia for tonal languages, with most participants being native speakers of Mandarin or Cantonese; North America for non-tonal languages, with most participants being English speakers), it remains unclear whether patterns of results across these studies reflect effects of tonal vs non-tonal language experience in general; effects of growing up in an East Asian vs Western culture; or some interaction between the two.

In this paper, we first assess the current degree of evidence, via meta-analysis, for an effect of tonal language experience on music processing. Then, we report new data from a massive online experiment that recruited a global sample, to directly measure the relation between linguistic experience and music perception across many tonal, non-tonal, and pitch-accented languages.

# Meta-analytic effects of tonal language experience on music processing ability

We aggregated summary information from `r study_total_num` prior studies of music perception and tonal language experience (see SI Text 1.1 for the search criteria) and studied them with random-effects meta-analysis models. Because there is no consensus in the literature about what specific music processing ability to expect a tonal-language advantage on, we grouped the prior studies into three rough categories: melody, fine-grained pitch, and rhythm. We built separate meta-analytic models for each group. 

The results are in Table 1. The overall effect size estimate suggests that native speakers of tonal languages have overall advantages in both categories of pitch-relevant tasks (melody processing: `r round(melodic.gen.random$TE.random, 3)`, 95% CI = [`r round(melodic.gen.random$lower.random,3)`, `r round(melodic.gen.random$upper.random,3)`]; fine-grained pitch processing: `r round(pitch.gen.random$TE.random, 3)`, 95% CI = [`r round(pitch.gen.random$lower.random,3)`, `r round(pitch.gen.random$upper.random,3)`]), but not in rhythm tasks (rhythm processing: `r round(rhythm.gen.random$TE.random, 3)`, 95% CI = [`r round(rhythm.gen.random$lower.random,3)`, `r round(rhythm.gen.random$upper.random,3)`]). The meta-analyses also identified three serious concerns, however, which preclude any generalized claim about the effects of tonal language experience on music processing ability.

```{r Table1}
#| fig.cap="\\textbf{Table 1.} Meta-analytic effects estimated for melody processing, fine-grained pitch processing, and rhythm processing, aggregated from 19 prior studies. Detailed information about the study and measures used, and the language composition of the participants in each study is included in the table. The figure embedded within the table shows the estimated effects for each study individually (modelled as random effects), with 95% confidence intervals indicated by the cross-bars. The bolded diamond shaped points represent the overall fixed-effect estimates for each of the three study categories (melody, fine-grained pitch, and rhythm)."


knitr::include_graphics(here("viz", "figures", "meta-plot.pdf"))
```


First, and most importantly, prior studies sample tonal languages narrowly, typically comparing Mandarin or Cantonese speakers from mainland China to English speakers from the United States. Of the tonal language speakers in prior studies, approximately 92% spoke Mandarin or Cantonese; and of the non-tonal speakers, approximately 85% spoke English. As such, no claim about the effects of language experience on music perception on the basis of the prior literature are justifiable, because it is not clear whether prior effects generalize beyond the select few highly studied languages.

Second, the majority of prior studies have very low statistical power due their small sample sizes, which may produce unreliable group-level estimates of effects and increase the risk of bias [@Kraemer1998]. We estimated the power of each prior study to detect effects of $d = 0.5$, a "medium" size effect comparable to the meta-analytic effect estimated for melodic discrimination tasks (Table 1). Across the three categories of music processing tasks, power was low (for studies of melodic discrimination, median power = `r round(median(melodic.gen.random$Power),2)`; for fine-grained pitch discrimination, median power = `r round(median(pitch.gen.random$Power),2)`; for rhythm tasks, median power = `r round(median(rhythm.gen.random$Power),2)`).

Third, participants' musical training experience has rarely been accounted for in the meta-analysed studies. At best, this contributes additional unsystematic variation within a sample, reducing statistical power. Access to musical training and the form of this musical training, however, may also vary systematically between countries [e.g., @Campbell2012], potentially leading to biased estimates of music perception abilities. Because the vast majority of participants in prior studies of tonal language experience and music perception ability include native speakers of two languages (i.e., Mandarin and Cantonese) from one country, this issue presents a substantial threat to the validity of prior findings: any systematic biases could produce effects erroneously attributed to differences in tonal language experience rather than cultural experience.

Thus, the meta-analysis demonstrates evidence for a potential effect of tonal language experience on melodic discrimination ability and finer-grained pitch discrimination ability — but a lack of linguistic diversity, low statistical power, and high likelihood of systematic bias in the samples studied warrant caution. These issues can be addressed by studying many native speakers of many languages, with and without music training experience, all of whom complete the same assessments of music processing ability. In the rest of the paper, we report a study of the ability to discriminate melodies, detect mistuned singing, and detect misaligned beats, in `r f(users$final)` people across the globe. Participants self-reported their native language, location, demographics, and degree of musical training, enabling language-wise and language-type-wise analyses of each of these musical abilities and their relations to linguistic and musical experience.

# Citizen-science experiment

Generalization from particular speakers of specific languages to entire groups of languages, with high statistical power, and with controls for potential sources of systematic bias, requires large samples of participants representing a variety of languages and countries [@Blasi2022; @Yarkoni2022]. We report such a test here, using methods of gamification and citizen science [@Hilton2022; @Huber2020; @Hartshorne2019; @Long2023].

## Methods

### Participants

Participants were visitors to the citizen-science website <https://themusiclab.org> who completed a set of three music perception tasks presented as an online game (*Test Your Musical IQ*). We did not recruit participants directly; rather, they visited the site after hearing about it organically (e.g., via Reddit posts, YouTube clips, Twitch streams). Participants gave informed consent under an ethics protocol approved either by the Harvard University Committee on the Use of Human Subjects (protocol IRB2017-1206) or the Yale University Human Research Protection Program (protocol 2000033433). Our analysis was preregistered and deviations are noted below. Please see “Data, code, and materials availability” section for more details.

```{r}
New_tonal <- c("Burmese", "Hmong", "Igbo", "Lao", "Yoruba", "Zulu", "Xhosa", "Ewe", "Shona", "Punjabi", "Kinyarwanda", "Bemba", "Twi", "Chinese/Other dialects")
```

We studied `r f(users$total)` participants who completed all three tasks, who had no missing or internally conflicting data, and who reported being a native speaker of one of the 40 most commonly spoken languages among all participants *or* were a native speaker of one of `r length(New_tonal)` tonal languages that have been less commonly studied in the context of music perception, such as Yoruba, Xhosa, and Lao (n.b., in the preregistration, we initially planned to only study speakers of the 40 most commonly spoken languages in the available data. However, based on advice from reviewers, we subsequently opted to include additional participants to increase the diversity of the tonal languages studied). Data were collected between Nov 8th, 2019 and Nov 12th, 2022.

We excluded participants that (a) had participated in the experiment on another occasion, to avoid any effects of learning ($n$ = `r f(excludes$full$ex_take)`); (b) reported a hearing impairment ($n$ = `r f(excludes$full$ex_hear)`); (c) reported their age as below 8 years or above 90 years ($n$ = `r f(excludes$full$ex_age)`); (d) reported an age of music lessons onset that was either below 2 years or above 90 years ($n$ = `r f(excludes$full$ex_lessonAge)`); (e) reported a music lesson onset age that was greater than their self-reported age ($n$ = `r f(excludes$full$ex_age_consist)`); (f) reported that they were participating in a noisy environment and were not wearing headphones ($n$ = `r f(excludes$full$ex_work)`; see SI Text 1.2 for analysis of a manipulation check to test whether participants were actually wearing headphones). `r f(excludes$full$ex_more_than_one)` participants were excluded for meeting more than one of the above criteria. The exclusion criteria were preregistered.

The resulting sample ($N$ = `r f(users$final)`; see Figure 1 and Table S1) included `r f(users$speakers$Tonal)` native speakers of `r f(users$langs$Tonal)` tonal languages; `r f(users$speakers$Pitch_accented)` native speakers of `r f(users$langs$Pitch_accented)` pitch-accented languages; and `r f(users$speakers$Non_tonal)` native speakers of `r f(users$langs$Non_tonal)` non-tonal languages. Languages were primarily classified based on the World Atlas of Language Structures [@Maddieson2013] and the Lyon-Albuquerque Phonological Systems Database [@Maddieson2014]. Languages that are not present in either database were classified according to information from the Phonetics Information Base and Lexicon Database [@Moran2019] or other sources from the linguistics literature. <!--No discrepancies were found across these sources. JL: WALS did classify 3 of the African languages as simple tone, which is unclear whether completely overlap with PA. I added a footnote to this in Table S1. Commenting out this statement unless others think it's warranted--> A summary of all languages studied here, with further classification details and language-wise sample sizes, is in Table S1.

In addition to participants' music perception scores, we collected demographic information (gender, age, whether or not the participant had taken music lessons, and the age at onset of those lessons). These data are reported in Table S2.

```{r Figure1}
#| fig.cap = "\\textbf{Figure 1.} Sample sizes for each language, grouped by language type. The font of each language's name is scaled proportionally to that language's sample size. Horizontal positions are jittered to improve readability. The right panel shows additional tonal languages with smaller sample sizes included in the analyses to increase the diversity of the tonal-language sample."
knitr::include_graphics(here("viz", "figures", "Figure1-1.pdf"))
```

### Stimuli

Participants completed three music perception tasks measuring ability in *melodic discrimination* [@Harrison2017], *mistuning perception* [@Larrouy-Maestri2019], and *beat alignment* [@Harrison2018]. The melodic discrimination task assesses the ability to detect differences between melodic patterns: participants listened to three transpositions of the same melody and were asked to choose the version in which one pitch interval was altered (i.e., an oddball task). The mistuning perception task assesses the ability to identify small-pitch differences in vocal pitch: participants listened to two versions of a short musical excerpt, one of which had a vocal track that was detuned from the background music, and were asked to identify the out-of-tune version. The beat alignment task assesses the ability to detect correct synchronization between a click track and some music: participants listened to two versions of the same musical excerpt, both accompanied by a click track; one of the click tracks was misaligned by a constant proportion and participants were asked which example was correctly aligned.

As in the original tasks cited above, each subtask was presented adaptively via `psychTestR` [@Harrison2020]. To minimize the duration of the experiment, we fixed the length of each subtask at 15 trials, the minimum number of trials with acceptably low mean standard errors, according to the original task designs. Demographic items were presented via `jsPsych` [@deLeeuw2015]. The citizen-science platform distributed the experiments using a modified pre-release version of `pushkin` [@Hartshorne2019]. Readers can try this implementation of the three music perception tasks at <https://themusiclab.org/quizzes/miq>.

### Analysis strategy

To test whether musical abilities differ reliably as a function of native language type (i.e., tonal vs. pitch-accented vs. non-tonal), we used mixed-effects linear regression adjusting for age, gender, whether the participant had taken music lessons (yes or no), and the interaction between language-type and music-lessons; with random-effects for language and country. Non-tonal language, female gender, and no-music-lessons were used as the reference levels for the fixed-effects. Stated in terms of {`lmer`} pseudo-code [@Bates2015], this model is as follows:

$$
\begin{aligned}
\begin{gathered}
\text{Performance} \sim \text{Language type} * \text{Music lessons} + \text{Gender} + \text{Age}\: + \\
(1|\text{language}) + (1|\text{country})
\end{gathered}
\end{aligned}
$$

The random-effect structure is particularly helpful for correcting for sampling imbalances, ensuring that no particular languages or countries can dominate the overall effect, while also allowing us to model variation across languages and countries directly (see, e.g., Figure 3). Note that this analysis approach deviates from our preregistered analysis plan, which involved applying a linear regression model at several sampling levels, without random effects. We made this change on the suggestion of a reviewer, and given the utility of mixed-effects models in measuring cultural variation [e.g., @Hilton2022]. For transparency, we report analyses and results from the preregistered approach in SI Text 1.2; these largely replicate the results reported in the next section.

## Results

### Tonal language experience shapes music processing

Native speakers of tonal languages had a reliable advantage in melodic discrimination compared to speakers of non-tonal and pitch-accented languages (Figure 2; full statistical reporting is in Table 2), with an effect size of substantive practical significance ($\beta$ = `r main_analysis_estimates$mdt$ToneTonal$beta`, $t$ = `r main_analysis_estimates$mdt$ToneTonal$t`, $p$ `r main_analysis_estimates$mdt$ToneTonal$p`), roughly half the size of the effect of having taken music lessons (an experience that one should reasonably expect to directly improve music perception ability). This result replicates the first meta-analytic result, previously shown mainly in Mandarin and Cantonese speakers (see Section 2), demonstrating that the tonal-language advantage for melodic discrimination generalizes to many *additional* tonal languages.

Contrasting with the prior meta-analytic effects, however, there was no clear advantage for tonal-language speakers on the vocal mistuning task ($\beta$ = `r main_analysis_estimates$mpt$ToneTonal$beta`, $t$ = `r main_analysis_estimates$mpt$ToneTonal$t`, $p$ = `r main_analysis_estimates$mpt$ToneTonal$p`; Table 2), despite the fact that the task *did* reliably demonstrate clear performance differences between those with and without music training ($\beta$ = `r main_analysis_estimates$mpt$musicLessonsYes$beta`, $t$ = `r main_analysis_estimates$mpt$musicLessonsYes$t`, $p$ `r main_analysis_estimates$mpt$musicLessonsYes$p`). In other words, while some experiences (e.g., musical training) do shape fine-grained pitch perception in the context of vocal mistuning, there seems to be minimal or no effect of tonal-language experience on this ability. This result contradicts prior studies of fine-grained pitch discrimination that reported tonal-language advantages in mainly Mandarin and Cantonese speakers.

Lastly, contrary the meta-analytic results, which found no effect of tonal language experience on rhythm perception, native speakers of tonal languages performed *worse* in beat perception relative to the non-tonal and pitch-accented groups ($\beta$ = `r main_analysis_estimates$cabat$ToneTonal$beta`, $t$ = `r main_analysis_estimates$cabat$ToneTonal$t`, $p$ `r main_analysis_estimates$cabat$ToneTonal$p`). Like the melodic discrimination effect, the size of this effect was large, approaching the size of effects of having taken music lessons (see Figure 2).

```{r Table2, results='asis'}

#cat("\\clearpage") # force table onto the start of its own page

kable(Table2, format = "latex", 
      escape = F, booktabs = T, linesep = "", longtable=T,
      col.names = c(
        "Term", "$\\beta$", "$SE$", "$t$", "$\\beta$", "$SE$", "$t$", "$\\beta$", "$SE$", "$t$"
      )) %>% 
  add_header_above(c(" ", "Melodic Discrimination" = 3, "Mistuning Perception" = 3, "Beat Alignment" = 3)) %>%
  kable_styling(position = "center", font_size = 7.5) %>% 
  row_spec(1, bold = T,  background = "#DCDCDC") %>%
  footnote(general = "Fixed-effects from random-effects models for each of the three musical tasks. ***$p < 0.001$, **$p < 0.01$, *$p$ < 0.05.",
           general_title = "Table 2.",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE)
```


```{r Figure2}
#| fig.cap = "\\textbf{Figure 2.} Native speakers of tonal languages have an advantage in melodic discrimination and a disadvantage in beat alignment. The solid dots show the estimated effects of language type, marginalizing over the average proportions for ages and gender, on each of the three music perception abilities tested, but without the estimated effects of having had music lessons. The additive marginal effects of having music lessons are displayed with the faded triangles, providing a comparison point for the language-type effect sizes. After marginalization, for ease of interpretability, a scalar transformation was applied to the coefficients such that the \"Non-tonal\" and \"No music lessons\" coefficients were equal to zero. Error-bars represent 95% confidence intervals of the mean. The dotted horizontal black line indicates the baseline (*y* = 0)."
knitr::include_graphics(here("viz", "figures", "Figure2-1.pdf"))
```


### Language-type effects are consistent across languages

To examine the degree to which the main effects held across the different tonal languages studied, we first examined language-wise estimates derived from the mixed-effects model. These showed a high degree of consistency in main effects within each language group (Figure 3). For example, on the melodic discrimination task, `r mods$languages$stats$mdt$sig` of `r mods$languages$stats$mdt$n` tonal language groups had an estimated advantage over the non-tonal language average ($ps$ < 0.05); whereas `r mods$languages$stats$cabat$sig` of `r mods$languages$stats$cabat$n` had an estimated disadvantage on the beat perception task ($ps$ < 0.05).

We then tested whether speakers of tonal and non-tonal languages could be distinguished on the basis of only their music perception scores, using a linear discriminant function. To compensate for the multi-level structure of our data (scores nested within languages nested within language types), rather than a standard discriminant function analysis, we used a non-parametric permutational approach [@Mundry2007] and ran it on the subset of the participants who indicated having not received musical training and who were native speakers of languages with a sample size of at least 500. From this dataset, we drew nested random samples of 5 tonal languages and 5 non-tonal languages; for each of the sampled languages, we sampled 100 participant scores with replacement and shuffled the assignment of whether that participant was marked "tonal" or "non-tonal". We trained a linear discriminant function on 30% of that sample, balancing across languages, and then used the trained model to predict whether the remaining 70% of the held-out sample was marked "tonal" or "non-tonal", yielding a percentage-correct score. We repeated the process 10,000 times to construct the null distribution for each of the three music perception tasks (see Supplementary Figure 2). To estimate the actual classification performance, the same nested sampling process was repeated 100 times for non-shuffled data, from which we obtained the mean proportions of correct classification for each task. 

The results robustly replicated the main findings. Tonal and non-tonal speakers could be reliably distinguished on average for both melodic discrimination ($p$ `r permutation_tests$T_NT$mdt$p %>% fp`) and beat alignment ($p$ `r permutation_tests$T_NT$cabat$p %>% fp`) tasks, but not for the mistuning perception task ($p$ `r permutation_tests$T_NT$mpt$p %>% fp`), thus convergently supporting the reliability of our main results. 

Last, we used the same linear discriminant analysis approach to compare pitch-accented to non-tonal language speakers (Supplementary Figure 3). Here, the approach failed to replicate the small advantage to pitch-accent language speakers on the beat alignment task ($p$ `r permutation_tests$PA_NT$cabat$p %>% fp`) identified by our mixed-effect modeling analysis (Table 2), but did find a small advantage for vocal mistuning ($p$ `r permutation_tests$PA_NT$mpt$p %>% fp`). We take this mixed result as cause to not interpret either effect for pitch-accent speakers, given both the less clear theoretical basis for such an effect and the noted ambiguity of the pitch-accented language type [@Hyman2009].

\newpage

\begin{sidewaysfigure}
    \centering
        \includegraphics{`r here("viz", "figures", "FigureTest-1.pdf")`}
    \caption{\textbf{Figure 3.} The main effects are consistent across 19 tonal languages. The forest plot displays the estimated average performance for each individual language, after adjusting for the effect of music lessons, age, and gender. The solid points denote random-effect estimates for each language, derived from the mixed-effects models reported in Table 2 and Figure 2; the error bars denote 95\% confidence intervals; and the text annotations specify the languages and language-wise sample sizes (in brackets).}
    \label{fig:awesome_image}
\end{sidewaysfigure}

\pagebreak

### Effects of tonal-language experience are not attributable to measured third variables

We tested whether the observed language-type effects were driven by systematic variability across participants of three types.

First, we tested whether the socioeconomic status of participants was associated with their task scores, as socioeconomic status may vary systematically with both country and native language in our data, and may mediate the opportunities a person has for musical development. For participants who reported living in the United States, we collected additional demographic information. To assess how socioeconomic status may mediate the main findings, we analysed this subset of participants who reported income information ($n$ = `r f(income_n)`). Despite being restricted to the United States, this sub-sample was nonetheless linguistically diverse, including `r f(users$income$Non_tonal$n)` non-tonal speakers of `r f(users$income$Non_tonal$n_langs)` languages, `r f(users$income$Tonal$n)` tonal speakers of `r f(users$income$Tonal$n_langs)` languages, and `r f(users$income$Pitch_accent$n)` speakers of pitch-accent languages of `r f(users$income$Pitch_accent$n_langs)` languages. We analysed these data using a mixed-effect model of the same structure used in the main analysis, except with an additional fixed-effect term for income. The results show that while income does positively predict performance, such that those with higher incomes tended to perform better, the tonal language melodic discrimination advantage and beat alignment disadvantage held robustly after accounting for these income effects (full statistical reporting is in Table S3).

Second, we examined whether systematic variation in education between speakers of different languages may have driven the observed differences in their performance. Among participants who reported their educational background ($n$ = `r f(education_n)`), we again conducted a mixed-effect analysis of the same structure as the main analysis with an additional fixed term for participants' education level. While education positively predicted performance on all three tasks, the main language-type effects replicated (Table S4). 

Last, we tested whether proximity to "Western" culture could explain the main findings, as the mistuning perception and beat alignment task stimuli used Western-style popular music; while globalized musical styles make this unlikely, it is possible that the degree of familiarity with this musical style systematically varies between tonal and non-tonal language speakers, which could confound the main findings. We compared the results of the main model in two subsets of the participants (combined $n$ = `r f(culture_n)`): those who resided in a primarily non-English-speaking Eastern country (China, Taiwan, Hong Kong, Thailand, Vietnam) vs. an English-speaking Western country (United States, United Kingdom, New Zealand, Australia, and Canada). The main language-type effects replicated and the results (Table S5) showed only a small, significant effect of region (West vs. East) on the mistuning perception task.

Taken together, these exploratory analyses show that while SES, education, and exposure to Western culture can correlate with participant performance, these potential confounders cannot account for the observed language-type effects reported here.

# Discussion

We found a clear link between linguistic experience and music processing abilities: native speakers of tonal languages performed better than native speakers of non-tonal languages on a task that required discriminating changes in melodic patterns and worse on a task requiring the perception of a beat. In each case, the effect size associated with being a tonal language speaker was roughly half as large as the effect of receiving music lessons, indicating an effect of substantive practical significance. There was no effect of language experience on the perception of fine-grained pitch, however, despite the fact that musical training was reliably associated with increased ability for this task. This is consistent with the fact that tonal languages do not require distinguishing fine-grained pitch patterns.

Our results are likely to generalize, given that they held across thousands of native speakers of 19 tonal languages, and hundreds of thousands of native speakers of 29 non-tonal languages, each sampled from over 100 countries around the world. The tonal languages include not only East Asian languages (e.g., Mandarin, Cantonese) commonly examined by prior studies, but also Southeast Asian and African languages (e.g., Burmese, Igbo, Shona) that have rarely or never been studied in the context of music perception research. The non-tonal languages in our study include speakers of languages as diverse as Arabic, Catalan, Hindi, and Ukrainian; along with non-tonal languages from South/Southeast Asia, such as Indonesian, Tagalog, and Malay. Inclusion of these Asian languages constitute an especially strong test, since their speakers may be more likely to share cultural similarities to speakers of tonal languages in those regions, thereby helping to rule out cultural confounds.

Our results also help clarify the previously mixed pattern of results concerning the effects of linguistic experience on music processing across different tasks and samples. For example, an advantage for tonal language speakers in melodic pattern processing is consistent with the majority of previous studies [@Alexander2008; @Bradley2016; @Choi2021; @Chen2016; @Swaminathan2021; @Wong2012], though not all [@Peretz2011; @Stevens2013; @Zheng2018]. Meanwhile, similar levels of performance on the fine-grained pitch task between tonal and non-tonal language speakers is supported by some prior studies [@Bent2006; @Bidelman2011a; @Stevens2013; @Tong2018; @Zheng2018] but not other studies that point to a tonal advantage/disadvantage on related tasks [@Bidelman2013; @Chang2016; @Giuliano2011; @Hutka2015; @Peretz2011; @Pfordresher2009]. Lastly, while rhythmic abilities in tonal language speakers have been less studied [see @Wong2012; @Zhang2020], a disadvantage in beat discrimination is consistent with recent work showing that tonal speakers give more weight to pitch cues than duration cues; this weighting cuts across auditory domains [@Jasmin2021]. By leveraging a consistent set of tasks and a large sample size, our results make clear that speaking a tonal language has a measurable connection to music perception ability — but not a uniform one, as this linguistic experience produces both positive and negative effects on perceptual abilities of different types.

Why might tonal language experience have these specific effects on music perception? The answer may lie in the shared mechanisms and neural processing resources associated with auditory perception, whether applied to language or music [@Patel2011; @Patel2008; @Kraus2010; @Asaridou2013; @Peretz2015; although see also @Asano2021; @Bidelman2013]. Both tonal languages and music rely on specialized pitch-based sound categories (tone contours or levels in speech; pitch motifs and melodies in music). If these categories are learned and processed through shared, domain-general mechanisms, then improving the efficiency of these mechanisms through practice in either domain should result in mutual improvements [@Asaridou2013 ;@Chang2016; @Delogu2006; @Delogu2010; @Patel2008; @McMullen2004]. This idea highlights differences between putatively domain-general auditory processing and putatively domain-specific mechanisms that underlie music processing [e.g., the processing of pitch in the context of a tonal hierarchy\; @Albouy2020; @Krumhansl2004; @Peretz2003; @Zatorre2002].

Our results do not explain, however, *how* these mechanisms might be improved by experience. One possibility is that language experience could shape domain-general perceptual strategies regarding inferences about high-level perceptual categories on the basis of low-level cues: acquired perceptual biases (i.e., from tonal language experience) may aid the processing of some stimuli while worsening the processing of others. In speech, listeners give more perceptual weight to cues that are more informative in discriminating contrasts that are salient in their native language [@Schertz2020; @Jasmin2021] and tonal language speakers rely more heavily on pitch to categorize and produce speech stress when acquiring a non-tonal L2 language compared to native speakers [@Wang2008; @Yu2010].

Similarly, people with pitch perception deficits learn to compensate for their deficits by giving more weight to durational cues when decoding speech prosody [@Jasmin2020b; @Jasmin2020]. Recent evidence suggests that similar effects emerge in music perception: Mandarin speakers have difficulty *ignoring* pitch cues relative to English and Spanish speakers, who have been found to more frequently make decisions based on duration cues [@Jasmin2021]. In turn, this is consistent with theories of the overlapping mechanisms of basic auditory perception [@Patel2011; @Patel2012b; @Wong2007; @Tierney2013; @Peretz2015]. Our findings unite these results and show their generality.

While the scope of our data collection allowed for analysis of music processing abilities in thousands of native speakers of six pitch-accented languages, the findings concerning these speakers were murky and failed to replicate across different analysis approaches. Within-language group variability was also high (see Figure 2) for pitch-accented languages, suggesting no common group advantage/disadvantage across speakers of pitch-accented languages. This, of course, is complicated by the inherently fuzzier nature of classification of pitch-accented languages, relative to tonal languages [see Footnote 1\; @Gussenhoven2004; @Hyman2006; @Hyman2009]. Further work that codifies that nature of both linguistic and musical pitch use across this set of generally understudied languages may provide more clarity. We encourage interested readers to re-analyze the open-access data reported here, using alternative classifications of languages into the "pitch-accented" category.

We note several other limitations. First, while we accounted for how much musical training participants had, we did not measure how long they engaged with this training, its intensity, or its type. As a result, our estimates of the effect of musical training have greater uncertainty (although the analyses for participants with no musical training, which largely replicate the main effects, help to mitigate this concern). Second, participants only reported their first language, so we were unable to examine the effects of bilingualism or multilingualism [@Krizman2012; @Liu2017; @Liu2020], nor assess whether fluently speaking both tonal and non-tonal languages (e.g. Mandarin and English) might have contributed additional variability in our results. Third, there are a host of other unmeasured cultural, environmental, and genetic factors that surely affect musical abilities. Moreover, these likely interact with each other, complicating causal inferences from the observational data we collected [see, e.g., recent findings that genetics and musical experience both influence linguistic tone perception in Cantonese\; @Wong2020]. These and other limitations will be best addressed through a variety of methodologies, including more targeted smaller scale approaches (including fieldwork) that complement the broad web-based citizen science approach used here.

In sum, our results show that across a range of geographic and demographic contexts, linguistic experience alters music perception ability in reliable (but not unitary) fashions. This implies that substantively different domains of auditory perception recruit at least some shared processing resources, which themselves are shaped by auditory experience.

\bigskip

# End notes {-}

## Supplemental information {-}

The supplemental information includes 3 text sections, 7 tables and 3 figures.

## Data, code, and materials availability {-}

A reproducible version of this manuscript, including all data and code, is available at <https://github.com/themusiclab/language-experience-music>. The preregistration is at <https://osf.io/xurdb>. Readers can try out the experiment at <https://themusiclab.org/quizzes/miq>; code for each of the three tasks is available at <https://github.com/pmcharrison/mpt>, <https://github.com/pmcharrison/mdt>, and <https://github.com/pmcharrison/cabat>.

## Acknowledgments {-}
This research was supported by the Duke University Internship Funding Program (J.L.); the Harvard Data Science Initiative (S.A.M.); and the National Institutes of Health Director's Early Independence Award DP5OD024566 (S.A.M. and C.B.H.). We thank the participants; P. Harrison and D. Müllensiefen for sharing code and assisting with the implementation of their music perception tasks; T. Bent, G. Bidelman, E. Bradley, A. Bradlow, D. Chang, W. Choi, R. Giuliano, M. Hove, S. Hutka, M. Ngo, S. Nguyen, I. Peretz, P. Pfordresher, S. X. Tong, S. Swaminathan, Y. Wang, N. Wicha, and L. Zhang for providing supplementary data and/or associated information for the meta-analysis; J. Simson for technical and research assistance; and the members of The Music Lab for discussion and feedback on the citizen-science platform, the experiment, and the manuscript.

## Author contributions {-}
- Conception: J.L. and S.A.M. 
- Experimental design and implementation: S.A.M. 
- Preregistration and planned analyses: J.L., C.B.H, E.B., and S.A.M. 
- Participant recruitment, data management, and data processing: S.A.M., C.B.H., and J.L.
- Analysis and visualization: J.L. and C.B.H., with contributions from E.B. and S.A.M. 
- Meta-analysis data collection: J.L.; Meta-analysis models: J.L. and C.B.H.
- Writing: J.L., C.B.H, E.B., and S.A.M.

\newpage

# Supplementary Information {-}

\setcounter{section}{0}

# Meta-analysis Procedure

## Selection criteria

We included in the meta-analysis only studies that examined the pitch-processing ability of native tonal and non-tonal language speakers via behavioral measures. We searched for studies on Google Scholar using the terms *(tone language OR tonal language) AND (musical pitch perception)* and inspected the first 200 results. In addition, we conducted forward and backward cross-referencing of @Asaridou2013, a review article on the link between musical and linguistic pitch. In the identified studies, we excluded those that (1) focused exclusively on absolute pitch, amusia, categorical perception, or cross-modal abilities (e.g. identifying visual representations of musical intervals); (2) studied only musicians; and/or (3) recruited only children under the age of 8. Only studies that were published and written in English were included. In all but one case, the participants studied were native speakers of the tonal language in question. The tonal language group in @Swaminathan2021 contained some non-native speakers; that study's inclusion or exclusion from the meta-analyses did not substantively affect the estimates of meta-analytic effect sizes, however. Lastly, we excluded one study [@Chang2016] that had a standardized effect size over an order of magnitude above the average effect size. This anomaly was due to suspiciously low standard-deviations of participant-level scores, seemingly due to an accidental shift of a decimal place. We have contacted the authors for clarification.

## Data processing and analysis

To mitigate the confounding effect of musical training, we excluded data from musicians when separate groups of musicians/non-musicians were recruited. The remaining studies that did not distinguish between musicians and non-musicians had participants with either minimal musical training or did not differ across tonal/non-tonal groups ^[For Jasmin et al. (2021), which studied several participants with lengthy exposure to musical training, we excluded those with more than three years of musical training and recalculated the relevant statistics]. Additionally, we removed measures regarding pitch memory and speed of processing. For the remaining pitch-relevant tasks, we classified them into two categories: melodic pattern discrimination and fine-tuned pitch discrimination. Melodic discrimination includes tasks that involve recognizing different note combinations, while fine-tuned pitch discrimination includes tasks that concerns discerning fine-grained pitch differences. Additionally, we aggregated rhythm-related tasks (mostly used as control measures) across the studies into a separate rhythm category. Our classification scheme results in 12 melodic discrimination effect sizes, 20 fine-tuned pitch discrimination effect sizes, and 7 rhythm effect sizes. Mean and standard deviation or Cohen's *d* were collected for all the relevant tasks. 

Data from studies/tasks that contained multiple tonal/non-tonal language groups (e.g., separate English and Korean groups) or reported their data at sub-task levels (e.g., 1/4 and 1/2 semitone for the pitch discrimination task) were further processed to produce a composite score for each tonal/non-tonal group or task. Specifically, for studies that contained multiple tonal/non-tonal groups, we used the formula $\bar{X}_{pooled} = \frac{n_1\bar{X}_{1} + n_2\bar{X}_2}{n_1 + n_2}$ and $SD_{pooled} = \sqrt{\frac{(n_{1}-1)SD_1^2 + (n_2-1)SD_2^2 + \frac{n_1n_2}{n_1+n_2}(\bar{X}_1-\bar{X}_2)^2}{(n_1+n_2-1)}}$ to calculate the combined mean and standard deviation for the group. Meanwhile, for studies that reported data at sub-task levels, we used the formula $\bar{X}_{pooled} = \frac{1}{n}\sum^n_{i=1}\bar{X}_i$ and $SD_{pooled} = \sqrt{(\frac{1}{n})^2(\sum^n_{i=1}Var_i + \sum_{i\neq j}(r_{ij}\sqrt{Var_i}\sqrt{Var_j})}$ (assuming correlation between the sub-task conditions equals 1, since they are targeting the same ability) to calculate a combined mean and standard deviation for the task for tonal and non-tonal language speakers [following @Borenstein2009]. This step produced one entry for each task, which we treat as our unit of analysis. 

From the mean and standard deviation, we calculated Hedge's *G* for each task and used it as our effect size measure. We then ran a random-effects model for each category of the tasks. \hl{Please refer to XX for commented code of the analysis.}

# Validation of self-reported headphone use

Participants who self-reported that they were wearing headphones completed a 6-trial headphone detection task [@Woods2017] designed to be easy for participants wearing headphones and difficult for those listening on free-field speakers. Out of the `r f(nrow(miq_combined_filtered %>% filter(headphone == "Yes")))` participants who indicated wearing headphones, `r f(nrow(headphone_score))` had clean and usable headphone detection data. The distribution of scores for these participants (Supplementary Figure 1) was strongly left-skewed with the median participant scoring `r round(mean(headphone_score$score),2)` of 6 (100%) correct. This implies that the bulk of participants who self-reported wearing headphones were, in fact, wearing headphones.

# Results from the preregistered analysis approach

In our preregistration (https://osf.io/xurdb), we specified an exploratory-confirmatory approach. For both sets of data, we planned OLS regression models exploratory ($n$ = `r f(users$explore)`) and confirmatory ($n$ = `r f(users$confirm)`) samples, controlling for age, gender, and music lesson (yes or no). In addition, to further reduce the confounding effect of covariates, we planned OLS regression models on three alternative samples, using different approaches to control for differences in music lesson experience, gender, and age (coarsened into 10 year bands). The three versions of the data were a 1:1 exactly matched sample, a 1:1 exactly matched sample with only participants who did not receive music lessons, and an inverse-probability weighted sample [@Austin2015; @Stuart2010]. The same simple linear regression model $Performance \sim Language\:type$ was planned for each.Results from the exploratory and confirmatory analyses are presented in Tables S6A & S6B, for transparency. The main findings from the exploratory dataset replicated in the confirmatory dataset. 

There are several limitations, however, that complicate the OLS results. First, the large sample size drove almost all effects to statistical significance in a way that may not translate to practical significance. Second, the imbalanced representation of different languages (e.g., dominance of English speakers in the non-tonal language group) may bias our estimates via cultural confounds endemic to the dominant languages in each group. These limitations motivated our adoption of mixed-effect models as the main analysis.

\clearpage

```{r TableS1, eval=T, warning=F, message=F}
languages <- read.csv(here("data", "language.csv"))

languages <- languages %>%
  mutate(language = case_when(
    language == "Chinese/Mandarin" ~ "Chinese/Mandarin",
    language == "Chinese/Cantonese/Yue" ~ "Chinese/Cantonese",
    language == "Chinese/Taiwanese/Min Nan/Hokkien" ~ "Chinese/Hokkien",
    TRUE ~ language
  )) 

languages <- languages %>%
  mutate(Type = case_when(
    Tone == "Tonal" ~ "Tonal",
    Tone == "Pitch_accent" ~ "Pitch-accented",
    TRUE ~ "Non-tonal"
  )) 

country <- miq_combined_filtered %>%
  group_by(language) %>%
  count(country) %>%
  arrange(desc(n)) %>%
  arrange(language) %>%
  filter(n == max(n)) %>%
  mutate(
    language = case_when(
      language == "Chinese/Mandarin" ~ "Chinese/Mandarin",
      language == "Chinese/Cantonese/Yue" ~ "Chinese/Cantonese",
      language == "Chinese/Taiwanese/Min Nan/Hokkien" ~ "Chinese/Hokkien",
      TRUE ~ language
    )) %>%
  select(language, country)

country_non_unique <- country %>% 
  count(language) %>%
  filter(n !=1) %>%
  pull(language)

if (length(country_non_unique)>0){
  
  for (x in country_non_unique) {
    idx <- which(country$language == x)
    country_combo = paste0(country$country[idx[1]], ", ", country$country[idx[2]])
    country[idx[1], 2] <- country_combo
    country <- country[-idx[2],]
    
  }
} 

TableS1 <- miq_combined_filtered %>%
  mutate(
  language = case_when(
    language == "Chinese/Mandarin" ~ "Chinese/Mandarin",
    language == "Chinese/Cantonese/Yue" ~ "Chinese/Cantonese",
    language == "Chinese/Taiwanese/Min Nan/Hokkien" ~ "Chinese/Hokkien",
    TRUE ~ language
  )) %>%
  count(language) %>%
  full_join(country) %>%
  full_join(languages %>%
              select(language, Tone, Source)) %>%
  arrange(desc(n)) %>%
  arrange(desc(Tone)) %>%
  select("Language" = language,
         "Language Type" = Tone,
         "Total $n$" = n,
         "Top country/region" = country,
         Source)

kable(TableS1, booktab = T, linesep = NULL, longtable = T, escape = F) %>%
  kable_styling(font_size = 6.5,
                 position = "center") %>%
  footnote(general = "The languages studied here, with sample sizes, largest-sample-size country, and the source of the language classification. Abbreviations: WALS (The world atlas of language structures), LAPSyD (Lyon-Albuquerque phonological systems database). *WALS classified as simple tone.",
           general_title = "Table S1.",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE)
```

\clearpage

```{r TableS2, eval=T}
# T: tonal, NT: non-tonal,PA: pitch-accent
Demo <- c("Gender", "Female", "Male", "Other", "Mean age (SD)", "Music lesson", "Yes", "No", "Mean age at onset of music lessons (SD)") 

# Gender:
Gender <- miq_combined_filtered %>%
  group_by(Tone)  %>%
  count(gender) %>%
  mutate(prop = n/sum(n))
  
T.female <- Gender[Gender$Tone == "Tonal" & Gender$gender == "Female",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
T.male <- Gender[Gender$Tone == "Tonal" & Gender$gender == "Male",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
T.other <- Gender[Gender$Tone == "Tonal" & Gender$gender == "Other",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
NT.female <- Gender[Gender$Tone == "Non-tonal" & Gender$gender == "Female",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
NT.male <- Gender[Gender$Tone == "Non-tonal" & Gender$gender == "Male",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
NT.other <- Gender[Gender$Tone == "Non-tonal" & Gender$gender == "Other",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
PA.female <- Gender[Gender$Tone == "Pitch-accented" & Gender$gender == "Female",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
PA.male <- Gender[Gender$Tone == "Pitch-accented" & Gender$gender == "Male",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
PA.other <- Gender[Gender$Tone == "Pitch-accented" & Gender$gender == "Other",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)

# Music Lessons
Lessons <- miq_combined_filtered %>%
  group_by(Tone)  %>%
  count(musicLessons) %>%
  mutate(prop = round(n/sum(n),2))

T.lessonNo <- Lessons[Lessons$Tone == "Tonal" & Lessons$musicLessons == "No",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
T.lessonYes <- Lessons[Lessons$Tone == "Tonal" & Lessons$musicLessons == "Yes",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
NT.lessonNo <- Lessons[Lessons$Tone == "Non-tonal" & Lessons$musicLessons == "No",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
NT.lessonYes <- Lessons[Lessons$Tone == "Non-tonal" & Lessons$musicLessons == "Yes",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
PA.lessonNo <- Lessons[Lessons$Tone == "Pitch-accented" & Lessons$musicLessons == "No",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)
PA.lessonYes <- Lessons[Lessons$Tone == "Pitch-accented" & Lessons$musicLessons == "Yes",]$prop %>% percent(suffix = "\\%", accuracy = 0.1)

# Age
Age <- miq_combined_filtered %>%
  group_by(Tone) %>%
  summarise(mean = round(mean(age),2), sd = round(sd(age),2)) %>%
  mutate(age = paste0(mean, " (", sd, ")"))

T.age <- Age[Age$Tone == "Tonal",]$age
NT.age <- Age[Age$Tone == "Non-tonal",]$age
PA.age <- Age[Age$Tone == "Pitch-accented",]$age

# Lesson Age
lesson.Age <- miq_combined_filtered %>%
  filter(!is.na(lessonsAge)) %>%
  group_by(Tone) %>%
  summarise(mean = round(mean(lessonsAge),2),
            sd = round(sd(lessonsAge),2))
NT.lessonAge <- paste0(lesson.Age[1,2], " (", lesson.Age[1,3], ")")
T.lessonAge <- paste0(lesson.Age[3,2], " (", lesson.Age[3,3], ")")
PA.lessonAge <- paste0(lesson.Age[2,2], " (", lesson.Age[2,3], ")")

Tone.demo <- c("", T.female, T.male, T.other, T.age, "", T.lessonYes, T.lessonNo, T.lessonAge)
NT.demo <- c("", NT.female, NT.male, NT.other, NT.age, "", NT.lessonYes, NT.lessonNo, NT.lessonAge)
PA.demo <- c("", PA.female, PA.male, PA.other, PA.age, "", PA.lessonYes, PA.lessonNo, PA.lessonAge)

TableS2 <- cbind(Demo, Tone.demo, NT.demo, PA.demo)
TableS2[1,1] <- cell_spec(TableS2[1,1], bold = T)
TableS2[5,1] <- cell_spec(TableS2[5,1], bold = T)
TableS2[6,1] <- cell_spec(TableS2[6,1], bold = T)
TableS2[9,1] <- cell_spec(TableS2[9,1], bold = T)

Con.lang.num <- miq_combined_filtered %>%
  count(Tone)

colnames(TableS2) <- c("Demographics", paste0("Tonal ($n$ = ", f(Con.lang.num[3,2]),")"), paste0("Non-tonal ($n$ = ", f(Con.lang.num[1,2]),")"), paste0("Pitch-accented ($n$ = ", f(Con.lang.num[2,2]),")"))
  
kable(TableS2, booktab = T, escape = F, linesep = NULL) %>%
  kable_styling(position = "center",
                font_size = 9) %>% 
  add_indent(positions = c(2,3,4,7,8)) %>% 
  column_spec(1, width = "12em")  %>% 
  footnote(general = "Demographics composition, by language type.",
           general_title = "Table S2.",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE) 
```

```{r TableS3}
kable(
  Table_combined_income,
  format = "latex", 
  escape = FALSE,
  booktabs = TRUE,
  linesep = "",
  longtable = TRUE,
  col.names = c(
    "Term", "$\\beta$", "$SE$", "$t$", "$\\beta$", "$SE$", "$t$", "$\\beta$", "$SE$", "$t$"
  )
) %>% 
  add_header_above(c(" ", "Melodic Discrimination" = 3, "Mistuning Perception" = 3, "Beat Alignment" = 3)) %>%
  kable_styling(position = "center", font_size = 7) %>% 
  row_spec(1, bold = T,  background = "#DCDCDC") %>%
  footnote(general = "Fixed-effect output from random-effects model applied to each musical task controlling for participant annual income (Baseline = Income under 10,000) in the US sample with available income data. Language type effect remains after controlling for income in this group. Language is included as random-effects. ***$p < 0.001$, **$p < 0.01, *$p < 0.05.",
           general_title = "Table S3.",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE)
```

\clearpage

```{r TableS4}
kable(
  Table_combined_education,
  format = "latex", 
  escape = FALSE,
  booktabs = TRUE,
  linesep = "",
  longtable = TRUE,
  col.names = c(
    "Term", "$\\beta$", "$SE$", "$t$", "$\\beta$", "$SE$", "$t$", "$\\beta$", "$SE$", "$t$"
  )
) %>% 
  add_header_above(c(" ", "Melodic Discrimination" = 3, "Mistuning Perception" = 3, "Beat Alignment" = 3)) %>%
  kable_styling(position = "center", font_size = 7) %>% 
  row_spec(1, bold = T,  background = "#DCDCDC") %>%
  footnote(general = "Fixed-effect output from random-effects model applied to each musical task controlling for education (Baseline = Primary School) in the sample that reported their educational background. Language and country are included as random-effects. ***$p < 0.001$, **$p < 0.01, *$p < 0.05.",
           general_title = "Table S4.",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE)
```


```{r TableS5}
kable(
  Table_combined_culture,
  format = "latex", 
  escape = FALSE,
  booktabs = TRUE,
  linesep = "",
  longtable = TRUE,
  col.names = c(
    "Term", "$\\beta$", "$SE$", "$t$", "$\\beta$", "$SE$", "$t$", "$\\beta$", "$SE$", "$t$"
  )
) %>% 
  add_header_above(c(" ", "Melodic Discrimination" = 3, "Mistuning Perception" = 3, "Beat Alignment" = 3)) %>%
  kable_styling(position = "center", font_size = 7.5) %>% 
  row_spec(1, bold = T,  background = "#DCDCDC") %>%
  footnote(general = "Fixed-effect output from random-effects model applied to each musical task controlling for region (East vs West, baseline = East). Language and country were entered as random-effects. Only speakers of tonal and non-tonal languages from countries/regions China, Hong Kong, Taiwan, Thailand, Vietnman (East) and United States, United Kingdom, New Zealand, Canada, Australia (West) are included in this analysis. ***$p < 0.001$, **$p < 0.01, *$p < 0.05.",
           general_title = "Table S5.",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE)
```

\clearpage

```{r TableS6}
kable(TableS2B, format = "latex", booktabs = T, linesep = "", escape = F) %>%
  kable_styling(position = "center",
                font_size = 9)  %>% 
  add_indent(positions = c(2,3,5,6,8,9,11,12))  %>% 
  footnote(general = "Summary of main results from linear regressions in the confirmatory sample, repeated across the four analysis approaches (*p < 0.05, ***p < 0.001).",
           general_title = "Table S6A.",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE)
```

```{r}
knitr::kable(TableS5B, booktabs = T, linesep = NULL, escape = F) %>%
  kable_styling(position = "center",
                font_size = 9)  %>% 
  add_indent(positions = c(2,3,5,6,8,9,11,12))  %>% 
  footnote(general =  "Beta coefficients from linear regression models for different analysis approaches in the exploratory sample (*p < 0.05, ***p < 0.001).",
           general_title = "Table S6B.",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE)
```


\clearpage
```{r FigureS1}
#| fig.cap = "\\textbf{Figure S1.} Scores on the headphone detection task, from participants who self-reported that they were wearing headphones. The maximum score was 6; the dashed red line indicates the mean score."
knitr::include_graphics(here("viz", "figures", "FigureS1-1.pdf"))
```


```{r FigureS2}
#| fig.cap = "\\textbf{Figure S2.} To test whether speakers of tonal and non-tonal languages could be distinguished on the basis of their music perception scores, we ran a permuted Discriminant Function Analysis (pDFA). The three histograms show the null distributions for each music perception task, representing the ability to discriminate 'tonal' from 'non-tonal' when these labels have been randomly shuffled, resulting from 10,000 permutations. The dotted vertical line then represents the actual discrimination performance on non-shuffled data. And the p-value is computed as a approximate test from this null distribution and actual score. "
knitr::include_graphics(here("viz", "figures", "FigureS2-1.pdf"))
```

```{r FigureS3}
#| fig.cap = "\\textbf{Figure S3.} To test whether speakers of pitch-accent and non-tonal languages could be distinguished on the basis of their music perception scores, we ran a permuted Discriminant Function Analysis (pDFA). The three histograms show the null distributions for each music perception task, representing the ability to discriminate 'tonal' from 'non-tonal' when these labels have been randomly shuffled, resulting from 10,000 permutations. The dotted vertical line then represents the actual discrimination performance on non-shuffled data. And the p-value is computed as a approximate test from this null distribution and actual score. "
knitr::include_graphics(here("viz", "figures", "FigureS3-1.pdf"))
```

\clearpage

# References {-}
